{
  "hash": "c57f66f8eb9d99e2c02268beafc9608b",
  "result": {
    "markdown": "---\ntitle: \"Day 2 - Web Scraping\"\nformat: html\n---\n\n\n# Introduction\n\nWeb scraping is a technique used to extract data from websites and organize it into a structured format for further analysis or use. It is an essential tool for data scientists, researchers, and businesses that rely on data from the web. There are two main approaches to web scraping: with and without an Application Programming Interface (API)\n\n**Static Websites vs. Dynamic Websites**\n\nBefore diving into web scraping, it's important to understand the difference between static and dynamic websites. Static websites are traditional websites that store all their content in an HTML document and present it when a request is made. The content of a static website is fixed and does not change unless the website is updated. On the other hand, dynamic websites use JavaScript to generate content on the fly, making them more interactive and responsive. This distinction is crucial because the approach to web scraping may differ depending on whether the website is static or dynamic.\n\n**Web Scraping without API**\n\nWeb scraping without an API involves directly accessing and extracting data from the HTML code of a website. This approach is suitable for static websites, where the content is fixed and can be accessed without any additional processing. Web scraping tools, such as web scrapers, are used to automate the data extraction process, making it faster and more efficient than manual copy-pasting.\n\n**Web Scraping with API**\n\nWeb scraping with an API involves using a set of definitions and communication protocols that connect a computer to a web server, allowing the user to access and extract data from a website. APIs provide direct access to specific data, often in a structured format like XML or JSON, making it easier to process and analyze. However, API access may be limited or costly, and not all websites offer APIs for data extraction.In summary, web scraping is a powerful technique for extracting data from websites, with different approaches depending on whether the website is static or dynamic and whether an API is available. As an empirical accounting professor, understanding these concepts will help you teach your students how to effectively scrape data from various websites for their research and analysis.\n\n# Webscraping without API\n\nIn this section we will download information and documents from [**https://www.annualreports.com**](https://www.annualreports.com/). Getting data from this website is exemplary for getting information from the web, where we don't have an API (Application Programming Interface) access.\n\n## Loading Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.3     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(rvest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n```\n:::\n\n```{.r .cell-code}\nlibrary(xml2)\nlibrary(robotstxt)\n```\n:::\n\n\n## Task 1: Understanding the Website\n\n**Objective**: Before we start scraping a website, it's crucial to understand its structure and any rules or restrictions it might have for web scrapers. This task will guide you through the process of examining the website and its **`robots.txt`** file.\n\n### Step 1: Visit the Website\n\nOpen your web browser and navigate to [**https://www.annualreports.com**](https://www.annualreports.com/). Familiarize yourself with its layout, content, and navigation. This will give you a visual understanding of where the data you want to scrape is located.\n\n![AnnualReport.com Startpage](/img/AnnualReportStartPage.png)\n\n### Step 2: Check the **`robots.txt`** File\n\nEvery website may have a **`robots.txt`** file. This file provides rules about which parts of the website can be accessed and scraped by web robots. It's essential to respect these rules to avoid any legal issues or getting banned from accessing the website.\n\nIn the script, the following code checks the **`robots.txt`** file of the website:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrobot_ <- robotstxt::robotstxt(\"https://www.annualreports.com\")\nrobot_$crawl_delay\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        field     useragent value\n1 Crawl-delay    semrushbot    15\n2 Crawl-delay SemrushBot-BA    15\n3 Crawl-delay        dotbot    10\n4 Crawl-delay       bingbot    10\n5 Crawl-delay     googlebot     1\n6 Crawl-delay             *    10\n```\n:::\n:::\n\n\n**Explanation**:\n\n-   The **`robotstxt::robotstxt()`** function fetches the **`robots.txt`** file from the website.\n\n-   **`robot_$crawl_delay`** checks if there's a specified delay between requests. This delay is recommended to avoid overloading the server with rapid, consecutive requests.\n\n### Step 3: Analyze the Results\n\nIf the **`crawl_delay`** value is provided, it's a good practice to incorporate this delay in your scraping script. This ensures you're sending requests at a rate the website server is comfortable with (since it is very long 10s, we will reduce this time for this class a bit).\n\n## Task 2: Scrape a list of all companies on annualreport.com\n\n**Objective**: After understanding the basics of the website in Task 1, the next step is to extract a list of companies from the website. This task will guide you through creating a function to scrape company details from **`annualreport.com`**.\n\n### Step 1: Identify the Data\n\nBefore writing the function, visit [**https://www.annualreports.com**](https://www.annualreports.com/) and identify where the company details are located. Notice the structure and patterns of the data, such as company names, links, industry names, and sector names.\n\n### Step 2: Write the `get_companies` Function\n\nYour task is to write a function named **`get_companies`** that will retrieve the company details from the website.\n\n**Input**:\n\n-   **`.n`**: A numeric value indicating the maximum number of companies to retrieve. Default is **`Inf`** (meaning it will try to retrieve all companies).\n\n**Output**: A dataframe with the following columns:\n\n-   **`company_name`**: The name of the company.\n\n-   **`company_link`**: The link to the company's page on **`annualreport.com`**.\n\n-   **`industry_name`**: The industry in which the company operates.\n\n-   **`sector_name`**: The sector in which the company operates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_companies <- function(.progress = TRUE, .n = Inf) {\n  # YOUR CODE HERE\n}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n### Step 3: Test the Function\n\nAfter writing the function, test it by retrieving a limited number of companies to ensure it works correctly. For instance, you can retrieve the details of the first 100 companies using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_companies <- get_companies(.n = 100)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n ============================>--   92% |  ETA:  0s\n```\n:::\n\n```{.r .cell-code}\ntab_companies\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 93 x 5\n   company_id                company_name company_link industry_name sector_name\n   <chr>                     <chr>        <chr>        <chr>         <chr>      \n 1 aeris-environmental       Aeris Envir~ https://www~ \"Pollution &~ Industrial~\n 2 scpharmaceuticals-inc     scPharmaceu~ https://www~ \"Biotechnolo~ Healthcare \n 3 us-concrete-inc           U.S. Concre~ https://www~ \"Cement\"      Industrial~\n 4 1-800-flowerscom          1-800-FLOWE~ https://www~ \"Specialty R~ Services   \n 5 10x-genomics-inc          10x Genomic~ https://www~ \"Healthcare ~ Healthcare \n 6 111-inc                   111, Inc.    https://www~ \"\"            Healthcare \n 7 degree                    1414 Degrees https://www~ \"\"            Utilities  \n 8 180-life-sciences-corp    180 Life Sc~ https://www~ \"Biotechnolo~ Healthcare \n 9 1847-holdings-llc         1847 Holdin~ https://www~ \"Conglomerat~ Industrial~\n10 1895-bancorp-of-wisconsi~ 1895 Bancor~ https://www~ \"Financial S~ Financial  \n# i 83 more rows\n```\n:::\n:::\n\n\n### Step 4: Review the Results\n\nExamine the **`tab_companies`** dataframe to ensure the data has been scraped correctly. Check if the company names, links, industry names, and sector names are correctly populated.\n\n## Download Raw HTMLs\n\n### Introduction\n\nWhen web scraping, it's often beneficial to download and store the raw HTML content of the web pages locally. This approach has several advantages:\n\n1.  **Reduced Server Load**: By saving the HTML locally, you avoid sending repeated requests to the website, which can overload their server or get your IP address banned.\n\n2.  **Offline Access**: Once downloaded, you can process the data without an active internet connection.\n\n3.  **Consistency**: The content of websites can change over time. By saving the HTML, you ensure you're always working with the same version of the data.\n\n### Functions\n\n#### 1. `save_raw_htmls()`\n\n**Purpose**: This function downloads and saves the HTML content of a specified webpage to a local directory.\n\n**Parameters**:\n\n-   **`.url`**: The URL of the webpage you want to download.\n\n-   **`.dir`**: The directory where you want to save the downloaded HTML.\n\n-   **`.format`**: The format in which to save the HTML. It can be either **`.fst`** (a dataframe format) or **`.html`** (standard HTML format). The **`.fst`** format can be advantageous because it allows for faster read/write operations and can store data in a more compressed form.\n\n-   **`.wait`**: The time (in seconds) to wait between requests. This is to ensure you're not sending requests too quickly, which could be seen as a potential attack on the server.\n\n**Output**: The function saves the HTML content to the specified directory and also creates a log file. This log file tracks details about the download, such as any errors encountered, the time taken, etc.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsave_raw_htmls <- function(.url, .dir, .format = c(\".fst\", \".html\"), .wait = 1) {\n  format_ <- match.arg(.format, c(\".fst\", \".html\"))\n  id_ <- basename(.url)\n\n  dir_ <- fs::dir_create(file.path(.dir, gsub(\"\\\\.\", \"\", format_)))\n  fil_ <- file.path(dir_, paste0(id_, format_))\n\n  if (file.exists(fil_)) {\n    return(NULL)\n  }\n\n  tab_log_ <- tibble::tibble(\n    company_id = id_,\n    company_link = .url,\n    format = .format,\n    path = fil_,\n    err = FALSE,\n    err_msg = NA_character_,\n    start_time = NA_real_,\n    time = NA_real_\n  )\n\n  tryCatch(\n    {\n      start_time_ <- Sys.time()\n      html_ <- rvest::read_html(.url)\n      end_time_ <- Sys.time()\n      Sys.sleep(.wait)\n      tab_log_ <- tab_log_ %>%\n        dplyr::mutate(\n          start_time = start_time_,\n          time = difftime(end_time_, start_time_, units = \"secs\")\n        )\n    },\n    error = function(e) {\n      tab_log_ <- dplyr::mutate(tab_log_, err = TRUE, err_msg = e$message)\n      return(tab_log_)\n    }\n  )\n\n  if (format_ == \".fst\") {\n    fst::write_fst(tibble::tibble(company_id = id_, html = as.character(html_)), fil_, 100)\n  } else {\n    write(as.character(html_), fil_)\n  }\n\n  if (!file.exists(file.path(.dir, \"log.csv\"))) {\n    readr::write_delim(tab_log_, file.path(.dir, \"log.csv\"), \"|\", na = \"\")\n  } else {\n    readr::write_delim(tab_log_, file.path(.dir, \"log.csv\"), \"|\", na = \"\", append = TRUE)\n  }\n}\n```\n:::\n\n\n#### 2. `read_raw_html()`\n\n**Purpose**: This function reads the locally stored HTML content and returns it as a node object, which can be further processed using functions from the **`rvest`** package.\n\n**Parameters**:\n\n-   **`.path`**: The path to the locally stored HTML file.\n\n-   **`.browse`**: A logical value. If set to **`TRUE`**, the function will also open the HTML in your default web browser. This can be useful for visually inspecting the content.\n\n**Output**: A node object containing the HTML content, which can be further processed or parsed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread_raw_html <- function(.path, .browse = FALSE) {\n  if (tools::file_ext(.path) == \"fst\") {\n    html_ <- rvest::read_html(fst::read_fst(.path, \"html\")[[\"html\"]])\n  } else {\n    html_ <- rvest::read_html(.url)\n  }\n\n  if (.browse) {\n    temp_file_ <- tempfile(fileext = \".html\")\n    write(as.character(html_), temp_file_)\n    browseURL(temp_file_)\n  }\n\n  return(html_)\n}\n```\n:::\n\n\n### Usage Tips\n\n1.  **Respect `robots.txt`**: Before using the **`save_raw_htmls()`** function, ensure you've checked the website's **`robots.txt`** file to ensure you're allowed to scrape it.\n\n2.  **Space Out Requests**: Use the **`.wait`** parameter in **`save_raw_htmls()`** to ensure you're not sending requests too quickly.\n\n3.  **Inspect Locally Saved HTML**: Use the **`.browse`** parameter in **`read_raw_html()`** to open the saved HTML in your browser. This can help you visually inspect the content and ensure it was saved correctly.\n\nRemember, web scraping requires a balance between gathering the data you need and respecting the website's server and terms of use. Always scrape responsibly!\n\n### Save HTMLs\n\n\n::: {.cell}\n\n```{.r .cell-code}\npurrr::walk(tab_companies$company_link, ~ save_raw_htmls(.x, \"day2/raw_html\", \".fst\"), .progress = TRUE)\n# purrr::walk(tab_companies$company_link, ~ save_raw_htmls(.x, \"day2/raw_html\", \".html\"), .progress = TRUE)\n```\n:::\n\n\n### Read the log file\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_log_companies <- readr::read_delim(\n  file = \"day2/raw_html/log.csv\",\n  delim = \"|\"\n) %>%\n  dplyr::filter(format == \".fst\")\ntab_log_companies\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 93 x 8\n   company_id  company_link format path  err   err_msg start_time           time\n   <chr>       <chr>        <chr>  <chr> <lgl> <lgl>   <dttm>              <dbl>\n 1 aeris-envi~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:30 0.476\n 2 scpharmace~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:31 1.14 \n 3 us-concret~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:33 1.10 \n 4 1-800-flow~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:36 0.415\n 5 10x-genomi~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:38 1.06 \n 6 111-inc     https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:40 1.15 \n 7 degree      https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:43 0.507\n 8 180-life-s~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:44 1.09 \n 9 1847-holdi~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:46 1.09 \n10 1895-banco~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:48 1.10 \n# i 83 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nread_raw_html(tab_log_companies$path[1], TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{html_document}\n<html lang=\"en\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n<div class=\"container\">\\n        <header class=\"header\"><div clas ...\n```\n:::\n:::\n\n\n## Task 3: Scrape all information for a single company\n\n**Objective**: After successfully scraping a list of companies, the next step is to delve deeper and extract detailed information for each individual company. This task will guide you through creating a function to scrape detailed company information from **`annualreport.com`**.\n\n### Step 1: Identify the Data\n\nBefore diving into the function, visit a specific company's page on [**https://www.annualreports.com**](https://www.annualreports.com/). Familiarize yourself with the layout and the various pieces of information available, such as the company name, ticker name, exchange name, number of employees, location, description, website, and social media links.\n\n### Step 2: Write the `get_company_info` Function\n\nYour task is to write a function named **`get_company_info`** that will retrieve detailed information about a company.\n\n**Input**:\n\n-   **`.html`**: A node object containing the HTML content of a company's page. This object can be obtained using the **`read_html`** function from the **`rvest`** package.\n\n**Output**: A dataframe with the following columns:\n\n-   **`vendor_name`**: The name of the company.\n\n-   **`ticker_name`**: The ticker symbol of the company.\n\n-   **`exchange_name`**: The stock exchange where the company is listed.\n\n-   **`employees`**: The number of employees in the company.\n\n-   **`location`**: The location or headquarters of the company.\n\n-   **`description`**: A brief description of the company.\n\n-   **`company_website`**: The official website of the company.\n\n-   **`facebook`**: The company's Facebook page link (if available).\n\n-   **`youtube`**: The company's YouTube channel link (if available).\n\n-   **`linkedin`**: The company's LinkedIn profile link (if available).\n\n-   **`twitter`**: The company's Twitter handle link (if available).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_company_info <- function(.html) {\n  # YOUR CODE HERE\n}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n### Step 3: Test the Function\n\nAfter writing the function, test it by passing the HTML content of a specific company's page to ensure it works correctly. For instance, you can retrieve the details of a company using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompany_details <- get_company_info(read_raw_html(tab_log_companies$path[1]))\ncompany_details\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 11\n  vendor_name         ticker_name exchange_name   employees location description\n  <chr>               <chr>       <chr>           <chr>     <chr>    <chr>      \n1 Aeris Environmental AEI         Exchange ASX M~ 11-50 Em~ Based i~ Aeris Envi~\n# i 5 more variables: company_website <chr>, facebook <chr>, youtube <chr>,\n#   linkedin <chr>, twitter <chr>\n```\n:::\n:::\n\n\n### Step 4: Review the Results\n\nExamine the **`company_details`** dataframe to ensure the data has been scraped correctly. Check if all the details like company name, ticker name, exchange name, etc., are correctly populated.\n\n## Task 4: Scrape the Annual Reports Links\n\n**Objective**: After extracting detailed information about individual companies, the next logical step is to gather their annual reports. This task will guide you through creating a function to scrape links to the annual reports of companies from **`annualreport.com`**.\n\n### Step 1: Identify the Data\n\nBefore diving into the function, visit a specific company's page on [**https://www.annualreports.com**](https://www.annualreports.com/). Familiarize yourself with the section that contains links to the company's annual reports. Notice the structure and patterns of the data, such as the year of the report and the download link.\n\n### Step 2: Write the `get_company_reports` Function\n\nYour task is to write a function named **`get_company_reports`** that will retrieve links to the annual reports of a company.\n\n**Input**:\n\n-   **`.html`**: A node object containing the HTML content of a company's page. This object can be obtained using the **`read_html`** function from the **`rvest`** package.\n\n**Output**: A dataframe with the following columns:\n\n-   **`heading`**: The title or heading of the annual report (e.g., \"2022 Annual Report\").\n\n-   **`year`**: The year of the annual report.\n\n-   **`report_link`**: The direct link to download the annual report.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_company_reports <- function(.html) {\n  # YOUR CODE HERE\n}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n### Step 3: Test the Function\n\nAfter writing the function, test it by passing the HTML content of a specific company's page to ensure it works correctly. For instance, you can retrieve the links to the annual reports of a company using:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport_links <- get_company_reports(read_raw_html(tab_log_companies$path[1]))\nreport_links\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 x 3\n  heading             year report_link                                          \n  <chr>              <int> <chr>                                                \n1 2021 Annual Report  2021 https://www.annualreports.com/HostedData/AnnualRepor~\n2 2020 Annual Report  2020 https://www.annualreports.com/HostedData/AnnualRepor~\n3 2019 Annual Report  2019 https://www.annualreports.com/HostedData/AnnualRepor~\n4 2018 Annual Report  2018 https://www.annualreports.com/HostedData/AnnualRepor~\n5 2017 Annual Report  2017 https://www.annualreports.com/HostedData/AnnualRepor~\n6 2016 Annual Report  2016 https://www.annualreports.com/HostedData/AnnualRepor~\n```\n:::\n:::\n\n\n### Step 4: Review the Results\n\nExamine the **`report_links`** dataframe to ensure the data has been scraped correctly. Check if all the details like the heading, year, and report link are correctly populated.\n\n## Get Company Information and Report Links\n\nIn this section, you're using the previously defined functions to extract detailed information about each company and the links to their annual reports.\n\n**Explanation**:\n\n1.  **`purrr::map`**: This function applies the **`get_company_info`** function to each path in **`tab_log_companies$path`**. The **`~`** symbol is used to define a formula where **`.x`** represents each individual path.\n\n2.  **`read_raw_html(.x)`**: For each path, the HTML content is read and passed to the **`get_company_info`** function.\n\n3.  **`dplyr::bind_rows`**: After extracting the information for each company, the results are combined into a single dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_companies_info <- purrr::map(tab_log_companies$path, ~ get_company_info(read_raw_html(.x)), .progress = TRUE)\ntab_companies_info <- dplyr::bind_rows(tab_companies_info)\ntab_companies_info\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 77 x 11\n   vendor_name          ticker_name exchange_name employees location description\n   <chr>                <chr>       <chr>         <chr>     <chr>    <chr>      \n 1 Aeris Environmental  AEI         Exchange ASX~ 11-50 Em~ Based i~ \"Aeris Env~\n 2 scPharmaceuticals I~ SCPH        Exchange NAS~ 11-50 Em~ Based i~ \"scPharmac~\n 3 U.S. Concrete, Inc.  RMIX        Exchange NAS~ 1001-500~ Based i~ \"U.S. Conc~\n 4 1-800-FLOWERS.COM    FLWS        Exchange NAS~ 1001-500~ Based i~ \"1-800-Flo~\n 5 10x Genomics, Inc.   TXG         Exchange NAS~ 501-1000~ Based i~ \"10x Genom~\n 6 111, Inc.            YI          Exchange NAS~ 1001-500~ Based i~ \"111, Inc.~\n 7 1414 Degrees         14D         Exchange ASX~ 11-50 Em~ Based i~ \"1414 Degr~\n 8 180 Life Sciences C~ ATNF        Exchange NAS~ 1-10 Emp~ Based i~ \"180 Life ~\n 9 1847 Holdings LLC    EFSH        Exchange NYS~ 1-10 Emp~ Based i~ \"1847 Hold~\n10 1895 Bancorp of Wis~ BCOW        Exchange NAS~ 51-200 E~ Based i~ \"1895 Banc~\n# i 67 more rows\n# i 5 more variables: company_website <chr>, facebook <chr>, youtube <chr>,\n#   linkedin <chr>, twitter <chr>\n```\n:::\n:::\n\n\n**Explanation**:\n\n-   This is similar to the previous block but focuses on extracting the annual report links for each company.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_company_reports <- purrr::map(tab_log_companies$path, ~ get_company_reports(read_raw_html(.x)), .progress = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n =======================>-------   77% |  ETA:  1s\n```\n:::\n\n```{.r .cell-code}\ntab_company_reports <- dplyr::bind_rows(tab_company_reports)\ntab_company_reports\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 842 x 3\n   heading             year report_link                                         \n   <chr>              <int> <chr>                                               \n 1 2021 Annual Report  2021 https://www.annualreports.com/HostedData/AnnualRepo~\n 2 2020 Annual Report  2020 https://www.annualreports.com/HostedData/AnnualRepo~\n 3 2019 Annual Report  2019 https://www.annualreports.com/HostedData/AnnualRepo~\n 4 2018 Annual Report  2018 https://www.annualreports.com/HostedData/AnnualRepo~\n 5 2017 Annual Report  2017 https://www.annualreports.com/HostedData/AnnualRepo~\n 6 2016 Annual Report  2016 https://www.annualreports.com/HostedData/AnnualRepo~\n 7 2021 Annual Report  2021 https://www.annualreports.com/HostedData/AnnualRepo~\n 8 2020 Annual Report  2020 https://www.annualreports.com/HostedData/AnnualRepo~\n 9 2019 Annual Report  2019 https://www.annualreports.com/HostedData/AnnualRepo~\n10 2018 Annual Report  2018 https://www.annualreports.com/HostedData/AnnualRepo~\n# i 832 more rows\n```\n:::\n:::\n\n\n## Download the Annual Reports\n\n**Explanation**:\n\n-   This function, **`download_pdf`**, is designed to download a PDF from a given URL and save it to a specified directory.\n\n-   It first checks if the file already exists in the directory. If it does, the function returns a message and doesn't download the file again.\n\n-   If the file doesn't exist, the function attempts to download the PDF. If the download is successful, details about the download (like the start time and duration) are logged in a tibble.\n\n-   If there's an error during the download (e.g., the URL doesn't point to a PDF), the error is caught and logged.\n\n-   The function also writes a log to a CSV file, which can be useful for tracking and debugging.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndownload_pdf <- function(.url, .dir, .wait = 1) {\n  # Create the directory if it doesn't exist\n  dir_ <- fs::dir_create(.dir, \"pdf\")\n  \n  # Extract the filename from the URL\n  id_ <- basename(.url)\n  fil_ <- file.path(dir_, id_)\n  \n  # Check if the file already exists\n  if (file.exists(fil_)) {\n    return(NULL)\n  }\n  \n  # Create a log tibble\n  tab_log_ <- tibble::tibble(\n    file_id = id_,\n    file_link = .url,\n    path = fil_,\n    err = FALSE,\n    err_msg = NA_character_,\n    start_time = NA_real_,\n    time = NA_real_\n  )\n  \n  # Try to download the PDF\n  tryCatch(\n    {\n      start_time_ <- Sys.time()\n      response <- httr::GET(.url, httr::write_disk(fil_, overwrite = TRUE))\n      \n      # Check if the response content type is a PDF\n      if (httr::http_type(response) != \"application/pdf\") {\n        stop(\"The URL does not point to a PDF.\")\n      }\n      \n      end_time_ <- Sys.time()\n      Sys.sleep(.wait)\n      tab_log_ <- tab_log_ %>%\n        dplyr::mutate(\n          start_time = start_time_,\n          time = difftime(end_time_, start_time_, units = \"secs\")\n        )\n    },\n    error = function(e) {\n      tab_log_ <- dplyr::mutate(tab_log_, err = TRUE, err_msg = e$message)\n      return(tab_log_)\n    }\n  )\n  \n  # Write the log to a CSV file\n  log_file <- file.path(.dir, \"log.csv\")\n  if (!file.exists(log_file)) {\n    readr::write_delim(tab_log_, log_file, \"|\", na = \"\")\n  } else {\n    readr::write_delim(tab_log_, log_file, \"|\", na = \"\", append = TRUE)\n  }\n}\n```\n:::\n\n\nDownload the first 100 reports\n\n\n::: {.cell}\n\n```{.r .cell-code}\npurrr::walk(tab_company_reports$report_link[1:100], ~ download_pdf(.x, \"day2/pdfs\"), .progress = TRUE)\n```\n:::\n\n\n**Explanation**:\n\n-   **`purrr::walk`** is used to apply the **`download_pdf`** function to the first 100 report links in **`tab_company_reports`**.\n\n-   Each report link is passed as **`.x`** to the **`download_pdf`** function, which then attempts to download the PDF and save it to the \"day2/pdfs\" directory.\n\n-   The **`.progress = TRUE`** argument provides a progress bar, which is helpful when downloading multiple files.\n\n# Webscraping with API\n\n**Task 1: Generate a Function to Download the SEC Master Index**\n\n1.  Navigate to the [**SEC EDGAR API documentation**](https://www.sec.gov/edgar/sec-api-documentation).\n\n2.  Ask your AI assistant to help you write a function in R that downloads the SEC master index for a specific year and quarter based on the API FAQ.\n\n3.  Test the function by downloading the master index for a year and quarter of your choice.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndownload_sec_master_index <- function(.year, .quarter, .dir) {\n  # YOUR CODE HERE\n\n}\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"day2/sec/master/master_2022_QTR1.idx\"\n```\n:::\n:::\n\n\n**Task 2: Inspect and Parse the Downloaded File**\n\n1.  Open the downloaded master index file and inspect its contents. Identify the structure and the data it contains.\n\n2.  Copy a small section (about 10-15 lines) of the master index.\n\n3.  Ask your AI assistant to parse the copied section into a dataframe in R.\n\n4.  Inspect the dataframe to ensure the data has been parsed correctly.\n\n::: {.cell}\n\n```{.r .cell-code}\nread_master_idx <- function(.path) {\n  # YOUR CODE HERE\n}\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 356,328 x 5\n   cik     company_name           form_type date_filed filename                 \n   <chr>   <chr>                  <chr>     <date>     <chr>                    \n 1 1000045 NICHOLAS FINANCIAL INC 10-Q      2022-02-09 edgar/data/1000045/00009~\n 2 1000045 NICHOLAS FINANCIAL INC 4         2022-02-03 edgar/data/1000045/00010~\n 3 1000045 NICHOLAS FINANCIAL INC 4         2022-02-03 edgar/data/1000045/00010~\n 4 1000045 NICHOLAS FINANCIAL INC 4         2022-02-11 edgar/data/1000045/00013~\n 5 1000045 NICHOLAS FINANCIAL INC 4         2022-02-18 edgar/data/1000045/00013~\n 6 1000045 NICHOLAS FINANCIAL INC 8-K       2022-01-13 edgar/data/1000045/00009~\n 7 1000045 NICHOLAS FINANCIAL INC 8-K       2022-01-31 edgar/data/1000045/00009~\n 8 1000045 NICHOLAS FINANCIAL INC SC 13G/A  2022-02-09 edgar/data/1000045/00011~\n 9 1000045 NICHOLAS FINANCIAL INC SC 13G    2022-02-08 edgar/data/1000045/00003~\n10 1000045 NICHOLAS FINANCIAL INC SC 13G    2022-02-11 edgar/data/1000045/00010~\n# i 356,318 more rows\n```\n:::\n:::\n\n\n**Task 3: Download a Full Submission Text File**\n\n1.  From the parsed dataframe in Task 2, identify a CIK (Central Index Key) and its associated year number for any company.\n\n2.  Ask your AI assistant to help you write a function in R that downloads a full submission 10-K text file using the CIK and year ( to filter that dataframe) and downloads the full submission text file.\n\n3.  Test the function by downloading the full submission text file for the identified company.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndownload_full_submision_file <- function(.tab, .cik, .year, .dir) {\n  # YOUR CODE HERE\n}\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n**Conclusion:**\n\nIn this exercise, you've learned how to leverage the capabilities of an AI assistant to interact with the SEC EDGAR database. By combining the power of AI with programming, you can efficiently access, parse, and analyze vast amounts of financial data. This approach not only simplifies the data extraction process but also opens up opportunities for more advanced financial analyses.\n\n**Bonus Challenge:**\n\n1.  Ask your AI assistant to help you extract specific sections (e.g., \"Item 1A. Risk Factors\") from the downloaded submission text files.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_section <- function(.path, .section = \"Item 1A. Risk Factors\") {\n  # Your Code Here\n}\n```\n:::\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}