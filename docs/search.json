[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nWelcome to the Mannheim Master in Management Analytics - Accounting Class of 2024 practical application sessions. Over the next two days, we will delve into the application of R and Excel to address prevalent business challenges, with a particular focus on web-scraping. Our aim is to equip you with insights into the automation and optimization of routine workflows.\nWhile the sessions are designed in R, a foundational understanding of programming is crucial. We will methodically break down the advanced techniques used in the code for better comprehension.\n\n\nResources and Assistance\nEngage in guided in-class exercises throughout these sessions. This year, we’re bolstered by the support of Large Language Models (LLMs). I strongly advocate for the utilization of these tools to amplify your learning experience. Here’s a snapshot of some leading LLMs:\n\nChatGPT (3.5/4.0): A distinguished tool in the AI domain.\nPerplexity AI: Harnessing the GPT-4 framework, it offers native web-searching capabilities.\nGoogle Bard: A notable AI tool in the market.\nClaude AI: Accessible primarily in US/UK; VPN can provide broader access.\n\nFor effective prompting, consider:\n\nPromptPerfect: Enhance your prompting skills.\nPromptoMania: An AI-driven prompt generator.\n\nDisclaimer:\nLLMs may produce diverse results. Some might provide direct solutions, while others may necessitate further adjustments. The primary goal is understanding, not necessarily mastering every coding detail. The overarching principle is: If it’s effective, it’s right!\n\n\nTable of Content\n\n\n\nDay\nDescription\n\n\n\n\nDay 1\nBuilding an Excel Business Case\nConstruct a concise business case in Excel, evaluating a business decisions using metrics like the Contribution Margin\n\n\nDay 2\nGetting Data from the Web\nMaster web scraping in scenarios with and without an API.\n\nhttp://www.annualreport.com/\nhttps://www.sec.gov/edgar\n\n\n\n\n\n\n\nInteresting Readings\n\nAdvanced R: https://adv-r.hadley.nz/\nR for Data Science: https://r4ds.had.co.nz/\nHands-On Programming With R: https://rstudio-education.github.io/hopr/"
  },
  {
    "objectID": "exercises/day1.html",
    "href": "exercises/day1.html",
    "title": "Day 1 - Building a Performance Measure Case",
    "section": "",
    "text": "Today’s complex business environment demands tools that offer both versatility and precision. In this session, we’ll explore three vital financial tasks using the R programming environment:\n\nCost Allocation: Get a granular understanding of how costs disperse across products, emphasizing the allocation of indirect costs.\nContribution Margin Analysis: Examine the profitability of individual products, distinguishing which products significantly impact the bottom line.\nMonte-Carlo Analysis: Navigate business uncertainties with this statistical technique, utilizing random sampling to achieve numerical results for multifaceted problems.\n\nBut why opt for R over mainstream tools like Excel?\nThe Case for R:\n\nReproducibility: With R scripts, analyses can be reproduced and validated effortlessly. This is invaluable for audits or simply revisiting your work months or years later.\nHandling Large Data sets: R is equipped to manage extensive datasets, something Excel struggles with beyond a point.\nFlexibility: R’s vast array of packages and its ability to integrate with other languages and tools make it highly adaptable.\nAdvanced Statistical Analysis: While Excel offers basic statistical tools, R provides a comprehensive suite for sophisticated analyses.\nCost-Effective: Being open-source, R is freely available, ensuring businesses don’t incur additional software costs.\n\nBy the session’s end, you’ll appreciate not just the theoretical aspects of these techniques but also the practical advantages of implementing them in an open-source environment like R.\nLet’s get started!"
  },
  {
    "objectID": "exercises/day1.html#task-1-allocate-indirect-cost",
    "href": "exercises/day1.html#task-1-allocate-indirect-cost",
    "title": "Day 1 - Building a Performance Measure Case",
    "section": "Task 1: Allocate Indirect Cost",
    "text": "Task 1: Allocate Indirect Cost\nWe know that we have to allocate indirect costs onto the individual product. Let’s tale Material Cost as an example:\n\nDirect Cost Elite (DC1): 30,000 €\nDirect Cost Premium (DC2): 70,000 €\nIndirect Cost (IC): 40,000\n\nThe general Formula for allocating indirect Cost is:\n\\[\nIC_{c,i} = IC_{c}*\\frac{DC_{c,i}}{\\sum_{i} DC_i}\n\\]\n\nTask 1A: Material Cost Allocation Preparation\nUse the given data to calculate the cost allocation:\n\nmat_dic &lt;- c(\"Elite\" = 30000, \"Premium\" = 70000)\nmat_inc &lt;- 40000\n\nToDo: Calculate the correct value for mat_ac\n\nmat_alc &lt;- NULL # YOUR CALCULATION HERE\n\n\n\n  Elite Premium \n  12000   28000 \n\n\nCheck Your Result\n\ncheck_named_vector(mat_alc, c(\"Elite\" = 12000, \"Premium\" = 28000))\n\nSuccess: Correct Values and Names\n\n\n\n\nTask 1B: Write a general cost allocation formula\nEnsure the function only accepts named numeric vectors.\nName conventions: .m_dic for the direct cost vector, and .s_inc for the indirect cost (m_ stands for multiple values and s_ stands for single values).\n\nallocate_indirect_cost &lt;- function(.m_dic, .s_inc) {\n  # YOUR CODE HERE\n}\n\n\n\n\n\nmat_alc &lt;- allocate_indirect_cost(mat_dic, mat_inc)\n\nCheck Your Result\n\ncheck_named_vector(mat_alc, c(\"Elite\" = 12000, \"Premium\" = 28000))\n\nSuccess: Correct Values and Names"
  },
  {
    "objectID": "exercises/day1.html#task-2-summing-cost-vectors",
    "href": "exercises/day1.html#task-2-summing-cost-vectors",
    "title": "Day 1 - Building a Performance Measure Case",
    "section": "Task 2: Summing Cost Vectors",
    "text": "Task 2: Summing Cost Vectors\nThe goal here is to devise a formula that can take multiple named numeric vectors as its input and return a new vector whose elements are the piece-wise sum of the elements in the input vectors. This formula will allow you to combine, for example, direct costs and allocated indirect costs to derive the total costs.\n\nTask 2A: Cost Summation Formula\nYou aim to create a function, sum_cost, that can receive any number of named numeric vectors and return their piece-wise sum.\nThe two common methods in R to handle an arbitrary number of function inputs are:\n\nUsing a list: Each vector is considered an item in the list. This method is more structured but requires each vector to be manually added to the list.\nUsing the ... operator: This is a placeholder for any number of arguments that can be passed to a function. It provides flexibility because you don’t have to specify in advance how many arguments you’re going to use. Later, inside the function, you can convert ... to a list using the list(...) function, allowing you to manipulate the arguments as a single list object.\n\nGiven these methods, here’s how you can create the sum_cost function:\n\nsum_cost &lt;- function(...) {\n  # YOUR CODE HERE\n}\n\n\n\n\n\nmat_tot &lt;- sum_cost(mat_dic, mat_inc )\n\nCheck Your Result\n\ncheck_named_vector(mat_tot, c(\"Elite\" = 70000, \"Premium\" = 110000))\n\nSuccess: Correct Values and Names"
  },
  {
    "objectID": "exercises/day1.html#task-3-using-real-data-for-cost-allocation",
    "href": "exercises/day1.html#task-3-using-real-data-for-cost-allocation",
    "title": "Day 1 - Building a Performance Measure Case",
    "section": "Task 3: Using Real Data for Cost Allocation",
    "text": "Task 3: Using Real Data for Cost Allocation\nTo understand and compute the cost allocation using the lecture data, we’ll extract it from an Excel sheet and perform specific operations based on the data’s structure.\n\ntab_cost &lt;- tibble::as_tibble(\n  openxlsx::read.xlsx(\"../Business Case.xlsx\", \"Cost1\")\n  ) %&gt;%\n  janitor::clean_names()\ntab_cost\n\n# A tibble: 9 x 6\n  type     product department    amount distr   range_std\n  &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 Direct   Elite   Material       30000 normal       5000\n2 Direct   Premium Material       70000 normal       7500\n3 Indirect &lt;NA&gt;    Material       40000 uniform      5000\n4 Direct   Elite   Manufacturing  40000 normal       2500\n5 Direct   Premium Manufacturing  50000 normal       1500\n6 Indirect &lt;NA&gt;    Manufacturing 135000 uniform     25000\n7 Indirect &lt;NA&gt;    Admin          73000 uniform      7500\n8 Indirect &lt;NA&gt;    Marketing      36500 uniform      3750\n9 Company  &lt;NA&gt;    &lt;NA&gt;           30000 uniform     22500\n\n\nHere, we use the openxlsx package to read the specific sheet “Cost1” from the Excel file. This data is then converted into a tibble (a modern form of R’s data frame) for easier manipulation. The clean_names() function from the janitor package is used to ensure our column names are consistent and easy to work with.\n\nTask 3A: Writing a formula to read product cost\nObjective: For this task, your goal is to write a function named filter_cost_input that filters the data for a specific cost type (e.g., Direct or Indirect) and a specific department (e.g., Material, Manufacturing). This function should return a named numeric vector where names are products, and values are the associated costs.\nYOUR FUNCTION:\n\nfilter_cost_input &lt;- function(.tab, .type, .dep) {\n  # YOUR CODE HERE\n}\n\nInstructions:\n\nUse the filter() function from the dplyr package to filter the table based on the given cost type and department.\nAfter filtering, arrange the results based on the product name. This will ensure consistency in later steps.\nNow, you’ll need to generate a named vector from the filtered results. This can be achieved using the mutate() function to set the names of the amount column to be the respective product names.\nFinally, extract just the named amount vector using the pull() function.\n\n\n\n\nCheck Your Result\n\nmat_dic &lt;- filter_cost_input(tab_cost, \"Direct\", \"Material\")\nmat_dic\n\n  Elite Premium \n  30000   70000 \n\n\n\ncheck_named_vector(mat_dic, c(\"Elite\" = 30000, \"Premium\" = 70000))\n\nSuccess: Correct Values and Names\n\n\n\n\nTask 3B: Writing formulas to get Cost Allocation List\nObjective: The aim of this exercise is to calculate different types of costs, both direct and indirect, and allocate them properly across products.\nStep-by-Step Guide:\n\nUnderstand the Structure: The get_cost_allocations function will compute various costs and store them in a list. This list will help organize and categorize costs according to their type and department.\nMaterial Costs:\n\nDirect Material Cost (mat_dic): This is already computed for you using the filter_cost_input function. It represents the costs directly linked to materials for each product.\nIndirect Material Cost (mat_inc): These costs are associated with materials but can’t be linked to a specific product. Instead, they’re spread out across all products.\nAllocated Material Cost (mat_alc): Here, you need to allocate the indirect material cost across all products based on some allocation rule (like proportion of direct costs or units produced). Use the allocate_indirect_cost function with mat_dic and mat_inc as arguments.\nTotal Material Cost (mat_tot): This is the sum of direct material cost and allocated indirect material cost. Use the sum_cost function.\n\nManufacturing Costs:\n\nDirect Manufacturing Cost (man_dic) and Indirect Manufacturing Cost (man_inc): These are computed in a manner similar to material costs.\nAllocated Manufacturing Cost (man_alc): Allocate the indirect manufacturing cost across products. Consider factors like machine hours or labor hours if relevant.\nTotal Manufacturing Cost (man_tot): Sum up the direct and allocated indirect manufacturing costs.\n\nAdministrative Costs:\n\nIndirect Admin Cost (adm_inc): As administrative costs are often indirect, allocate these across products based on a suitable allocation base (e.g., total costs or sales).\nAllocated Admin Cost (adm_alc): Spread the indirect admin cost using an appropriate rule.\n\nMarketing Costs:\n\nIndirect Marketing Cost (mar_inc): These are the costs associated with marketing activities that can’t be directly linked to a product.\nAllocated Marketing Cost (mar_alc): Distribute the marketing costs among products.\n\nTotal Product Cost (prd_tot): This is a summation of all costs associated with a product - material, manufacturing, administrative, and marketing. Ensure all costs are considered, both direct and allocated indirect.\n\nUse the Functions allocate_indirect_cost() and sum_cost() to complete the get_cost_allocations() functions\nYOUR FUNCTION:\n\nget_cost_allocations &lt;- function(.tab_cost) {\n  # Create an empty list to store values\n  lst_ &lt;- list()\n\n  # Material Cost\n  lst_[[\"mat_dic\"]] &lt;- filter_cost_input(.tab_cost, \"Direct\", \"Material\")   # Direct Material Cost\n  lst_[[\"mat_inc\"]] &lt;- filter_cost_input(.tab_cost, \"Indirect\", \"Material\") # Indirect Material Cost\n  lst_[[\"mat_alc\"]] &lt;- NULL # YOUR CODE HERE                                # Allocated Material Cost\n  lst_[[\"mat_tot\"]] &lt;- NULL # YOUR CODE HERE                                # Total Material Cost\n\n  # Manufacturing Cost\n  lst_[[\"man_dic\"]] &lt;- filter_cost_input(.tab_cost, \"Direct\", \"Manufacturing\")   # Direct Manufacturing Cost\n  lst_[[\"man_inc\"]] &lt;- filter_cost_input(.tab_cost, \"Indirect\", \"Manufacturing\") # Indirect Manufacturing Cost\n  lst_[[\"man_alc\"]] &lt;- NULL # YOUR CODE HERE                                     # Allocated Manufacturing Cost\n  lst_[[\"man_tot\"]] &lt;- NULL # YOUR CODE HERE                                     # Total Manufacturing Cost\n\n  # Administrative Cost\n  lst_[[\"adm_inc\"]] &lt;- filter_cost_input(.tab_cost, \"Indirect\", \"Admin\") # Indirect Admin Cost\n  lst_[[\"adm_alc\"]] &lt;- NULL # YOUR CODE HERE                             # Allocated Admin Cost\n\n  # Marketing Cost\n  lst_[[\"mar_inc\"]] &lt;- filter_cost_input(.tab_cost, \"Indirect\", \"Marketing\") # Indirect Marketing Cost\n  lst_[[\"mar_alc\"]] &lt;- NULL # YOUR CODE HERE                                 # Allocated Marketing Cost\n\n  # Total Product Cost\n  lst_[[\"prd_tot\"]] &lt;- NULL # YOUR CODE HERE\n\n  return(lst_)\n}\n\n\n\n\nFunction Breakdown:\n\nFunction Name: get_cost_allocation_skeleton\n\nThis name suggests that we are fetching or creating a foundational structure for our cost allocations.\n\nThe tibble::tribble Function:\n\nThis function is from the tibble package and helps in creating a small table (or tibble) in a readable form.\nThe ~sign and ~var are column names in this table. Each subsequent line in the function provides a row of data for these columns.\n\n\nUsage:\nWhen you call the function, it returns the described tibble structure. This “skeleton” serves as a roadmap for allocating and tallying up various costs. As you proceed with the exercise, you will populate this table with actual monetary values corresponding to each cost type. This visual representation helps in comprehending the flow and accumulation of different cost components.\n\nget_cost_allocation_skeleton &lt;- function() {\n  tibble::tribble(\n    ~sign, ~var,\n    \" \", \"Direct Material Costs\",\n    \"+\", \"Indirect Material Costs\",\n    \"=\", \"Material Costs\",\n    \"+\", \"Direct Manufacturing Costs\",\n    \"+\", \"Indirect Manufacturing Costs\",\n    \"=\", \"Manufacturing Costs\",\n    \"+\", \"Indirect Administration Costs\",\n    \"+\", \"Indirect Marketing Costs\",\n    \"=\", \"Total Product Costs\"\n  )\n}\n\n\nget_cost_allocation_skeleton()\n\n# A tibble: 9 x 2\n  sign  var                          \n  &lt;chr&gt; &lt;chr&gt;                        \n1 \" \"   Direct Material Costs        \n2 \"+\"   Indirect Material Costs      \n3 \"=\"   Material Costs               \n4 \"+\"   Direct Manufacturing Costs   \n5 \"+\"   Indirect Manufacturing Costs \n6 \"=\"   Manufacturing Costs          \n7 \"+\"   Indirect Administration Costs\n8 \"+\"   Indirect Marketing Costs     \n9 \"=\"   Total Product Costs          \n\n\n\n\nTask 3C: Display Cost Allocation Table\nIn this task, your goal is to use the pre-defined functions get_cost_allocation_list() and get_cost_allocation_skeleton() to construct the final Cost Allocation Table.\nThe Cost Allocation Table is crucial because it breaks down both direct and indirect costs, allowing a company to understand its cost structure better. By the end of this task, you should be able to represent all these costs in a well-organized table.\nInstructions:\n\nUnderstanding the Skeleton:\n\nBefore jumping into building the entire table, let’s familiarize ourselves with the get_cost_allocation_skeleton() function. This function provides a structured, step-by-step representation of how costs are compiled. Each row of the output represents either an individual cost or a summation.\nTry running the function by itself to see its output!\n\nFetching the Costs:\n\nThe get_cost_allocation_list() function is designed to extract various types of costs from the dataset. It returns a list containing costs categorized by their type (direct, indirect) and nature (material, manufacturing, etc.).\nFor the purpose of this task, run the function with your dataset and examine its output. Familiarize yourself with the different types of costs it provides.\n\nBuilding the Table:\n\nYour main challenge is to construct the Cost Allocation Table. This will involve integrating the structure provided by the skeleton with the actual cost data.\nThink of how you can combine the structured format of the skeleton with the actual cost numbers. You’ll need to fetch each relevant cost from the list and place it in the correct position within the table.\nNote: The skeleton uses signs like “+” and “=” to signify addition and summation. This can guide you on where and how to place and calculate costs.\n\nSteps to Consider for the Function:\n\nStart by fetching the costs using get_cost_allocation_list().\nUse the output of get_cost_allocation_skeleton() as your foundational table.\nFor each line in the skeleton, fetch the corresponding cost from your list and append it as a new column to the table.\nOnce you’ve placed all the individual costs, think about the rows that require summation (those with “=”). Calculate these using the appropriate rows.\nContinue this process until you’ve constructed the entire table.\nAdd a Total Column to the Table that shows the total cost over all Products\n\nTesting:\n\nOnce you’ve written your function, test it with the given data. Your output should align with the structure of the skeleton and display all the relevant costs.\n\n\nTips:\n\nRemember, you’re not starting from scratch! You have a structured format and categorized data. It’s all about bringing them together in the right order.\nThe use of functions from packages like dplyr and purrr can significantly simplify data manipulation and transformation. If you’re familiar with these, consider how they can assist in your task.\n\n\nget_cost_allocation_table &lt;- function(.tab_cost) {\n  # YOUR CODE HERE\n}\n\n\n\n\n\nget_cost_allocation_table(.tab_cost = tab_cost)\n\n# A tibble: 9 x 5\n  sign  var                            Total  Elite Premium\n  &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 \" \"   Direct Material Costs         100000  30000   70000\n2 \"+\"   Indirect Material Costs        40000  12000   28000\n3 \"=\"   Material Costs                140000  42000   98000\n4 \"+\"   Direct Manufacturing Costs     90000  40000   50000\n5 \"+\"   Indirect Manufacturing Costs  135000  60000   75000\n6 \"=\"   Manufacturing Costs           365000 142000  223000\n7 \"+\"   Indirect Administration Costs  73000  28400   44600\n8 \"+\"   Indirect Marketing Costs       36500  14200   22300\n9 \"=\"   Total Product Costs           474500 184600  289900\n\n\n\nformat_table(get_cost_allocation_table(tab_cost))\n\n\n\n\n\n  \n    \n    \n      sign\n      var\n      Total\n      Elite\n      Premium\n    \n  \n  \n     \nDirect Material Costs\n100,000\n30,000\n70,000\n    +\nIndirect Material Costs\n40,000\n12,000\n28,000\n    =\nMaterial Costs\n140,000\n42,000\n98,000\n    +\nDirect Manufacturing Costs\n90,000\n40,000\n50,000\n    +\nIndirect Manufacturing Costs\n135,000\n60,000\n75,000\n    =\nManufacturing Costs\n365,000\n142,000\n223,000\n    +\nIndirect Administration Costs\n73,000\n28,400\n44,600\n    +\nIndirect Marketing Costs\n36,500\n14,200\n22,300\n    =\nTotal Product Costs\n474,500\n184,600\n289,900"
  },
  {
    "objectID": "exercises/day1.html#task-4a-subtracting-cost-vectors-for-revenue-vectors",
    "href": "exercises/day1.html#task-4a-subtracting-cost-vectors-for-revenue-vectors",
    "title": "Day 1 - Building a Performance Measure Case",
    "section": "Task 4A: Subtracting Cost Vectors for Revenue Vectors",
    "text": "Task 4A: Subtracting Cost Vectors for Revenue Vectors\nYou aim to create a function, substract_cost, that can receive any number of named numeric vectors and subtract values from the first value.\nThe two common methods in R to handle an arbitrary number of function inputs are:\n\nUsing a list: Each vector is considered an item in the list. This method is more structured but requires each vector to be manually added to the list.\nUsing the ... operator: This is a placeholder for any number of arguments that can be passed to a function. It provides flexibility because you don’t have to specify in advance how many arguments you’re going to use. Later, inside the function, you can convert ... to a list using the list(...) function, allowing you to manipulate the arguments as a single list object.\n\nGiven these methods, here’s how you can create the substract_cost function:\n\nsubtract_cost &lt;- function(...) {\n  # YOUR CODE HERE\n}\n\n\n\n\nCheck Your Result\n\nrevenues &lt;- c(\"Elite\" = 100000, \"Premium\" = 250000)\nvar_cost &lt;- c(\"Elite\" = 70000, \"Premium\" = 120000)\n\ncm1 &lt;- subtract_cost(revenues, var_cost)\n\n\ncheck_named_vector(cm1, c(\"Elite\" = 30000, \"Premium\" = 130000))\n\nSuccess: Correct Values and Names"
  },
  {
    "objectID": "exercises/day1.html#task-4b-writing-a-formula-to-read-product-revenues",
    "href": "exercises/day1.html#task-4b-writing-a-formula-to-read-product-revenues",
    "title": "Day 1 - Building a Performance Measure Case",
    "section": "Task 4B: Writing a formula to read product revenues",
    "text": "Task 4B: Writing a formula to read product revenues\nObjective: For this task, your goal is to write a function named filter_revenue_input This function should return a named numeric vector where names are products, and values are the associated costs.\nYOUR FUNCTION:\n\nfilter_revenue_input &lt;- function(.tab, .type, .dep) {\n  # YOUR CODE HERE\n}\n\n\n\n\n\nfilter_revenue_input(tab_revenue)\n\n  Elite Premium \n 175000  300000"
  },
  {
    "objectID": "exercises/day1.html#task-4c-writing-formulas-to-get-contribution-margin-list",
    "href": "exercises/day1.html#task-4c-writing-formulas-to-get-contribution-margin-list",
    "title": "Day 1 - Building a Performance Measure Case",
    "section": "Task 4C: Writing formulas to get Contribution Margin List",
    "text": "Task 4C: Writing formulas to get Contribution Margin List\nRemember Task 3B. Use the Functions subtract_cost() and sum_cost() to complete the get_cost_allocations_and_list() function.\n\nget_cost_allocation_and_cm_list &lt;- function(.tab_cost, .tab_revenue) {\n  lst_cost_ &lt;- get_cost_allocation_list(.tab_cost) # The Cost Allocation List\n  lst_cm_   &lt;- list() # A new List for Contribution Margin\n\n  lst_cm_[[\"revenues\"]] &lt;- filter_revenue_input(.tab_revenue) # Revenues\n  lst_cm_[[\"var_cost\"]] &lt;- NULL # YOUR CODE HERE    \n  lst_cm_[[\"cm1\"]]      &lt;- NULL # YOUR CODE HERE    \n  lst_cm_[[\"fix_cost\"]] &lt;- NULL # YOUR CODE HERE    \n  lst_cm_[[\"cm2\"]]      &lt;- NULL # YOUR CODE HERE    \n  \n  return(c(lst_cost_, lst_cm_)) # Return the combined list\n}"
  },
  {
    "objectID": "exercises/day1.html#task-4d-display-contribution-margin-table",
    "href": "exercises/day1.html#task-4d-display-contribution-margin-table",
    "title": "Day 1 - Building a Performance Measure Case",
    "section": "Task 4D: Display Contribution Margin Table",
    "text": "Task 4D: Display Contribution Margin Table\nRemember Task 3C.\n\nget_contribution_margin_skeleton &lt;- function() {\n  tibble::tribble(\n    ~sign, ~var,\n    \" \", \"Revenue\",\n    \"-\", \"Variable Cost\",\n    \"=\", \"Contribution Margin 1\",\n    \"-\", \"Product Fix Cost\",\n    \"=\", \"Contribution Margin 2\",\n  )\n}\n\n\nmake_cm_table &lt;- function(.tab_cost, .tab_revenue) {\n  # YOUR CODE HERE\n}\n\n\n\n\n\nformat_table(make_cm_table(tab_cost, tab_revenue))\n\n\n\n\n\n  \n    \n    \n      sign\n      var\n      Total\n      Elite\n      Premium\n    \n  \n  \n     \nRevenue\n475,000\n175,000\n300,000\n    -\nVariable Cost\n190,000\n70,000\n120,000\n    =\nContribution Margin 1\n285,000\n105,000\n180,000\n    -\nProduct Fix Cost\n284,500\n114,600\n169,900\n    =\nContribution Margin 2\n500\n(9,600)\n10,100"
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "Curriculum vitae",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "exercises/day2.html",
    "href": "exercises/day2.html",
    "title": "Day 2 - Web Scraping",
    "section": "",
    "text": "Web scraping is a technique used to extract data from websites and organize it into a structured format for further analysis or use. It is an essential tool for data scientists, researchers, and businesses that rely on data from the web. There are two main approaches to web scraping: with and without an Application Programming Interface (API)\nStatic Websites vs. Dynamic Websites\nBefore diving into web scraping, it’s important to understand the difference between static and dynamic websites. Static websites are traditional websites that store all their content in an HTML document and present it when a request is made. The content of a static website is fixed and does not change unless the website is updated. On the other hand, dynamic websites use JavaScript to generate content on the fly, making them more interactive and responsive. This distinction is crucial because the approach to web scraping may differ depending on whether the website is static or dynamic.\nWeb Scraping without API\nWeb scraping without an API involves directly accessing and extracting data from the HTML code of a website. This approach is suitable for static websites, where the content is fixed and can be accessed without any additional processing. Web scraping tools, such as web scrapers, are used to automate the data extraction process, making it faster and more efficient than manual copy-pasting.\nWeb Scraping with API\nWeb scraping with an API involves using a set of definitions and communication protocols that connect a computer to a web server, allowing the user to access and extract data from a website. APIs provide direct access to specific data, often in a structured format like XML or JSON, making it easier to process and analyze. However, API access may be limited or costly, and not all websites offer APIs for data extraction.In summary, web scraping is a powerful technique for extracting data from websites, with different approaches depending on whether the website is static or dynamic and whether an API is available. As an empirical accounting professor, understanding these concepts will help you teach your students how to effectively scrape data from various websites for their research and analysis."
  },
  {
    "objectID": "exercises/day2.html#loading-libraries",
    "href": "exercises/day2.html#loading-libraries",
    "title": "Day 2 - Web Scraping",
    "section": "Loading Libraries",
    "text": "Loading Libraries\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.2     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.3     v tibble    3.2.1\nv lubridate 1.9.2     v tidyr     1.3.0\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(rvest)\n\n\nAttaching package: 'rvest'\n\nThe following object is masked from 'package:readr':\n\n    guess_encoding\n\nlibrary(xml2)\nlibrary(robotstxt)"
  },
  {
    "objectID": "exercises/day2.html#task-1-understanding-the-website",
    "href": "exercises/day2.html#task-1-understanding-the-website",
    "title": "Day 2 - Web Scraping",
    "section": "Task 1: Understanding the Website",
    "text": "Task 1: Understanding the Website\nObjective: Before we start scraping a website, it’s crucial to understand its structure and any rules or restrictions it might have for web scrapers. This task will guide you through the process of examining the website and its robots.txt file.\n\nStep 1: Visit the Website\nOpen your web browser and navigate to https://www.annualreports.com. Familiarize yourself with its layout, content, and navigation. This will give you a visual understanding of where the data you want to scrape is located.\n\n\n\nAnnualReport.com Startpage\n\n\n\n\nStep 2: Check the robots.txt File\nEvery website may have a robots.txt file. This file provides rules about which parts of the website can be accessed and scraped by web robots. It’s essential to respect these rules to avoid any legal issues or getting banned from accessing the website.\nIn the script, the following code checks the robots.txt file of the website:\n\nrobot_ &lt;- robotstxt::robotstxt(\"https://www.annualreports.com\")\nrobot_$crawl_delay\n\n        field     useragent value\n1 Crawl-delay    semrushbot    15\n2 Crawl-delay SemrushBot-BA    15\n3 Crawl-delay        dotbot    10\n4 Crawl-delay       bingbot    10\n5 Crawl-delay     googlebot     1\n6 Crawl-delay             *    10\n\n\nExplanation:\n\nThe robotstxt::robotstxt() function fetches the robots.txt file from the website.\nrobot_$crawl_delay checks if there’s a specified delay between requests. This delay is recommended to avoid overloading the server with rapid, consecutive requests.\n\n\n\nStep 3: Analyze the Results\nIf the crawl_delay value is provided, it’s a good practice to incorporate this delay in your scraping script. This ensures you’re sending requests at a rate the website server is comfortable with (since it is very long 10s, we will reduce this time for this class a bit)."
  },
  {
    "objectID": "exercises/day2.html#task-2-scrape-a-list-of-all-companies-on-annualreport.com",
    "href": "exercises/day2.html#task-2-scrape-a-list-of-all-companies-on-annualreport.com",
    "title": "Day 2 - Web Scraping",
    "section": "Task 2: Scrape a list of all companies on annualreport.com",
    "text": "Task 2: Scrape a list of all companies on annualreport.com\nObjective: After understanding the basics of the website in Task 1, the next step is to extract a list of companies from the website. This task will guide you through creating a function to scrape company details from annualreport.com.\n\nStep 1: Identify the Data\nBefore writing the function, visit https://www.annualreports.com and identify where the company details are located. Notice the structure and patterns of the data, such as company names, links, industry names, and sector names.\n\n\nStep 2: Write the get_companies Function\nYour task is to write a function named get_companies that will retrieve the company details from the website.\nInput:\n\n.n: A numeric value indicating the maximum number of companies to retrieve. Default is Inf (meaning it will try to retrieve all companies).\n\nOutput: A dataframe with the following columns:\n\ncompany_name: The name of the company.\ncompany_link: The link to the company’s page on annualreport.com.\nindustry_name: The industry in which the company operates.\nsector_name: The sector in which the company operates.\n\n\nget_companies &lt;- function(.progress = TRUE, .n = Inf) {\n  # YOUR CODE HERE\n}\n\n\n\n\n\n\nStep 3: Test the Function\nAfter writing the function, test it by retrieving a limited number of companies to ensure it works correctly. For instance, you can retrieve the details of the first 100 companies using:\n\ntab_companies &lt;- get_companies(.n = 100)\n\n ============================&gt;--   92% |  ETA:  0s\n\ntab_companies\n\n# A tibble: 93 x 5\n   company_id                company_name company_link industry_name sector_name\n   &lt;chr&gt;                     &lt;chr&gt;        &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;      \n 1 aeris-environmental       Aeris Envir~ https://www~ \"Pollution &~ Industrial~\n 2 scpharmaceuticals-inc     scPharmaceu~ https://www~ \"Biotechnolo~ Healthcare \n 3 us-concrete-inc           U.S. Concre~ https://www~ \"Cement\"      Industrial~\n 4 1-800-flowerscom          1-800-FLOWE~ https://www~ \"Specialty R~ Services   \n 5 10x-genomics-inc          10x Genomic~ https://www~ \"Healthcare ~ Healthcare \n 6 111-inc                   111, Inc.    https://www~ \"\"            Healthcare \n 7 degree                    1414 Degrees https://www~ \"\"            Utilities  \n 8 180-life-sciences-corp    180 Life Sc~ https://www~ \"Biotechnolo~ Healthcare \n 9 1847-holdings-llc         1847 Holdin~ https://www~ \"Conglomerat~ Industrial~\n10 1895-bancorp-of-wisconsi~ 1895 Bancor~ https://www~ \"Financial S~ Financial  \n# i 83 more rows\n\n\n\n\nStep 4: Review the Results\nExamine the tab_companies dataframe to ensure the data has been scraped correctly. Check if the company names, links, industry names, and sector names are correctly populated."
  },
  {
    "objectID": "exercises/day2.html#download-raw-htmls",
    "href": "exercises/day2.html#download-raw-htmls",
    "title": "Day 2 - Web Scraping",
    "section": "Download Raw HTMLs",
    "text": "Download Raw HTMLs\n\nIntroduction\nWhen web scraping, it’s often beneficial to download and store the raw HTML content of the web pages locally. This approach has several advantages:\n\nReduced Server Load: By saving the HTML locally, you avoid sending repeated requests to the website, which can overload their server or get your IP address banned.\nOffline Access: Once downloaded, you can process the data without an active internet connection.\nConsistency: The content of websites can change over time. By saving the HTML, you ensure you’re always working with the same version of the data.\n\n\n\nFunctions\n\n1. save_raw_htmls()\nPurpose: This function downloads and saves the HTML content of a specified webpage to a local directory.\nParameters:\n\n.url: The URL of the webpage you want to download.\n.dir: The directory where you want to save the downloaded HTML.\n.format: The format in which to save the HTML. It can be either .fst (a dataframe format) or .html (standard HTML format). The .fst format can be advantageous because it allows for faster read/write operations and can store data in a more compressed form.\n.wait: The time (in seconds) to wait between requests. This is to ensure you’re not sending requests too quickly, which could be seen as a potential attack on the server.\n\nOutput: The function saves the HTML content to the specified directory and also creates a log file. This log file tracks details about the download, such as any errors encountered, the time taken, etc.\n\nsave_raw_htmls &lt;- function(.url, .dir, .format = c(\".fst\", \".html\"), .wait = 1) {\n  format_ &lt;- match.arg(.format, c(\".fst\", \".html\"))\n  id_ &lt;- basename(.url)\n\n  dir_ &lt;- fs::dir_create(file.path(.dir, gsub(\"\\\\.\", \"\", format_)))\n  fil_ &lt;- file.path(dir_, paste0(id_, format_))\n\n  if (file.exists(fil_)) {\n    return(NULL)\n  }\n\n  tab_log_ &lt;- tibble::tibble(\n    company_id = id_,\n    company_link = .url,\n    format = .format,\n    path = fil_,\n    err = FALSE,\n    err_msg = NA_character_,\n    start_time = NA_real_,\n    time = NA_real_\n  )\n\n  tryCatch(\n    {\n      start_time_ &lt;- Sys.time()\n      html_ &lt;- rvest::read_html(.url)\n      end_time_ &lt;- Sys.time()\n      Sys.sleep(.wait)\n      tab_log_ &lt;- tab_log_ %&gt;%\n        dplyr::mutate(\n          start_time = start_time_,\n          time = difftime(end_time_, start_time_, units = \"secs\")\n        )\n    },\n    error = function(e) {\n      tab_log_ &lt;- dplyr::mutate(tab_log_, err = TRUE, err_msg = e$message)\n      return(tab_log_)\n    }\n  )\n\n  if (format_ == \".fst\") {\n    fst::write_fst(tibble::tibble(company_id = id_, html = as.character(html_)), fil_, 100)\n  } else {\n    write(as.character(html_), fil_)\n  }\n\n  if (!file.exists(file.path(.dir, \"log.csv\"))) {\n    readr::write_delim(tab_log_, file.path(.dir, \"log.csv\"), \"|\", na = \"\")\n  } else {\n    readr::write_delim(tab_log_, file.path(.dir, \"log.csv\"), \"|\", na = \"\", append = TRUE)\n  }\n}\n\n\n\n2. read_raw_html()\nPurpose: This function reads the locally stored HTML content and returns it as a node object, which can be further processed using functions from the rvest package.\nParameters:\n\n.path: The path to the locally stored HTML file.\n.browse: A logical value. If set to TRUE, the function will also open the HTML in your default web browser. This can be useful for visually inspecting the content.\n\nOutput: A node object containing the HTML content, which can be further processed or parsed.\n\nread_raw_html &lt;- function(.path, .browse = FALSE) {\n  if (tools::file_ext(.path) == \"fst\") {\n    html_ &lt;- rvest::read_html(fst::read_fst(.path, \"html\")[[\"html\"]])\n  } else {\n    html_ &lt;- rvest::read_html(.url)\n  }\n\n  if (.browse) {\n    temp_file_ &lt;- tempfile(fileext = \".html\")\n    write(as.character(html_), temp_file_)\n    browseURL(temp_file_)\n  }\n\n  return(html_)\n}\n\n\n\n\nUsage Tips\n\nRespect robots.txt: Before using the save_raw_htmls() function, ensure you’ve checked the website’s robots.txt file to ensure you’re allowed to scrape it.\nSpace Out Requests: Use the .wait parameter in save_raw_htmls() to ensure you’re not sending requests too quickly.\nInspect Locally Saved HTML: Use the .browse parameter in read_raw_html() to open the saved HTML in your browser. This can help you visually inspect the content and ensure it was saved correctly.\n\nRemember, web scraping requires a balance between gathering the data you need and respecting the website’s server and terms of use. Always scrape responsibly!\n\n\nSave HTMLs\n\npurrr::walk(tab_companies$company_link, ~ save_raw_htmls(.x, \"day2/raw_html\", \".fst\"), .progress = TRUE)\n# purrr::walk(tab_companies$company_link, ~ save_raw_htmls(.x, \"day2/raw_html\", \".html\"), .progress = TRUE)\n\n\n\nRead the log file\n\ntab_log_companies &lt;- readr::read_delim(\n  file = \"day2/raw_html/log.csv\",\n  delim = \"|\"\n) %&gt;%\n  dplyr::filter(format == \".fst\")\ntab_log_companies\n\n# A tibble: 93 x 8\n   company_id  company_link format path  err   err_msg start_time           time\n   &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt;   &lt;dttm&gt;              &lt;dbl&gt;\n 1 aeris-envi~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:30 0.476\n 2 scpharmace~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:31 1.14 \n 3 us-concret~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:33 1.10 \n 4 1-800-flow~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:36 0.415\n 5 10x-genomi~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:38 1.06 \n 6 111-inc     https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:40 1.15 \n 7 degree      https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:43 0.507\n 8 180-life-s~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:44 1.09 \n 9 1847-holdi~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:46 1.09 \n10 1895-banco~ https://www~ .fst   day2~ FALSE NA      2023-08-22 10:49:48 1.10 \n# i 83 more rows\n\n\n\nread_raw_html(tab_log_companies$path[1], TRUE)\n\n{html_document}\n&lt;html lang=\"en\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\n&lt;div class=\"container\"&gt;\\n        &lt;header class=\"header\"&gt;&lt;div clas ..."
  },
  {
    "objectID": "exercises/day2.html#task-3-scrape-all-information-for-a-single-company",
    "href": "exercises/day2.html#task-3-scrape-all-information-for-a-single-company",
    "title": "Day 2 - Web Scraping",
    "section": "Task 3: Scrape all information for a single company",
    "text": "Task 3: Scrape all information for a single company\nObjective: After successfully scraping a list of companies, the next step is to delve deeper and extract detailed information for each individual company. This task will guide you through creating a function to scrape detailed company information from annualreport.com.\n\nStep 1: Identify the Data\nBefore diving into the function, visit a specific company’s page on https://www.annualreports.com. Familiarize yourself with the layout and the various pieces of information available, such as the company name, ticker name, exchange name, number of employees, location, description, website, and social media links.\n\n\nStep 2: Write the get_company_info Function\nYour task is to write a function named get_company_info that will retrieve detailed information about a company.\nInput:\n\n.html: A node object containing the HTML content of a company’s page. This object can be obtained using the read_html function from the rvest package.\n\nOutput: A dataframe with the following columns:\n\nvendor_name: The name of the company.\nticker_name: The ticker symbol of the company.\nexchange_name: The stock exchange where the company is listed.\nemployees: The number of employees in the company.\nlocation: The location or headquarters of the company.\ndescription: A brief description of the company.\ncompany_website: The official website of the company.\nfacebook: The company’s Facebook page link (if available).\nyoutube: The company’s YouTube channel link (if available).\nlinkedin: The company’s LinkedIn profile link (if available).\ntwitter: The company’s Twitter handle link (if available).\n\n\nget_company_info &lt;- function(.html) {\n  # YOUR CODE HERE\n}\n\n\n\n\n\n\nStep 3: Test the Function\nAfter writing the function, test it by passing the HTML content of a specific company’s page to ensure it works correctly. For instance, you can retrieve the details of a company using:\n\ncompany_details &lt;- get_company_info(read_raw_html(tab_log_companies$path[1]))\ncompany_details\n\n# A tibble: 1 x 11\n  vendor_name         ticker_name exchange_name   employees location description\n  &lt;chr&gt;               &lt;chr&gt;       &lt;chr&gt;           &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;      \n1 Aeris Environmental AEI         Exchange ASX M~ 11-50 Em~ Based i~ Aeris Envi~\n# i 5 more variables: company_website &lt;chr&gt;, facebook &lt;chr&gt;, youtube &lt;chr&gt;,\n#   linkedin &lt;chr&gt;, twitter &lt;chr&gt;\n\n\n\n\nStep 4: Review the Results\nExamine the company_details dataframe to ensure the data has been scraped correctly. Check if all the details like company name, ticker name, exchange name, etc., are correctly populated."
  },
  {
    "objectID": "exercises/day2.html#task-4-scrape-the-annual-reports-links",
    "href": "exercises/day2.html#task-4-scrape-the-annual-reports-links",
    "title": "Day 2 - Web Scraping",
    "section": "Task 4: Scrape the Annual Reports Links",
    "text": "Task 4: Scrape the Annual Reports Links\nObjective: After extracting detailed information about individual companies, the next logical step is to gather their annual reports. This task will guide you through creating a function to scrape links to the annual reports of companies from annualreport.com.\n\nStep 1: Identify the Data\nBefore diving into the function, visit a specific company’s page on https://www.annualreports.com. Familiarize yourself with the section that contains links to the company’s annual reports. Notice the structure and patterns of the data, such as the year of the report and the download link.\n\n\nStep 2: Write the get_company_reports Function\nYour task is to write a function named get_company_reports that will retrieve links to the annual reports of a company.\nInput:\n\n.html: A node object containing the HTML content of a company’s page. This object can be obtained using the read_html function from the rvest package.\n\nOutput: A dataframe with the following columns:\n\nheading: The title or heading of the annual report (e.g., “2022 Annual Report”).\nyear: The year of the annual report.\nreport_link: The direct link to download the annual report.\n\n\nget_company_reports &lt;- function(.html) {\n  # YOUR CODE HERE\n}\n\n\n\n\n\n\nStep 3: Test the Function\nAfter writing the function, test it by passing the HTML content of a specific company’s page to ensure it works correctly. For instance, you can retrieve the links to the annual reports of a company using:\n\nreport_links &lt;- get_company_reports(read_raw_html(tab_log_companies$path[1]))\nreport_links\n\n# A tibble: 6 x 3\n  heading             year report_link                                          \n  &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;                                                \n1 2021 Annual Report  2021 https://www.annualreports.com/HostedData/AnnualRepor~\n2 2020 Annual Report  2020 https://www.annualreports.com/HostedData/AnnualRepor~\n3 2019 Annual Report  2019 https://www.annualreports.com/HostedData/AnnualRepor~\n4 2018 Annual Report  2018 https://www.annualreports.com/HostedData/AnnualRepor~\n5 2017 Annual Report  2017 https://www.annualreports.com/HostedData/AnnualRepor~\n6 2016 Annual Report  2016 https://www.annualreports.com/HostedData/AnnualRepor~\n\n\n\n\nStep 4: Review the Results\nExamine the report_links dataframe to ensure the data has been scraped correctly. Check if all the details like the heading, year, and report link are correctly populated."
  },
  {
    "objectID": "exercises/day2.html#get-company-information-and-report-links",
    "href": "exercises/day2.html#get-company-information-and-report-links",
    "title": "Day 2 - Web Scraping",
    "section": "Get Company Information and Report Links",
    "text": "Get Company Information and Report Links\nIn this section, you’re using the previously defined functions to extract detailed information about each company and the links to their annual reports.\nExplanation:\n\npurrr::map: This function applies the get_company_info function to each path in tab_log_companies$path. The ~ symbol is used to define a formula where .x represents each individual path.\nread_raw_html(.x): For each path, the HTML content is read and passed to the get_company_info function.\ndplyr::bind_rows: After extracting the information for each company, the results are combined into a single dataframe.\n\n\ntab_companies_info &lt;- purrr::map(tab_log_companies$path, ~ get_company_info(read_raw_html(.x)), .progress = TRUE)\ntab_companies_info &lt;- dplyr::bind_rows(tab_companies_info)\ntab_companies_info\n\n# A tibble: 77 x 11\n   vendor_name          ticker_name exchange_name employees location description\n   &lt;chr&gt;                &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;      \n 1 Aeris Environmental  AEI         Exchange ASX~ 11-50 Em~ Based i~ \"Aeris Env~\n 2 scPharmaceuticals I~ SCPH        Exchange NAS~ 11-50 Em~ Based i~ \"scPharmac~\n 3 U.S. Concrete, Inc.  RMIX        Exchange NAS~ 1001-500~ Based i~ \"U.S. Conc~\n 4 1-800-FLOWERS.COM    FLWS        Exchange NAS~ 1001-500~ Based i~ \"1-800-Flo~\n 5 10x Genomics, Inc.   TXG         Exchange NAS~ 501-1000~ Based i~ \"10x Genom~\n 6 111, Inc.            YI          Exchange NAS~ 1001-500~ Based i~ \"111, Inc.~\n 7 1414 Degrees         14D         Exchange ASX~ 11-50 Em~ Based i~ \"1414 Degr~\n 8 180 Life Sciences C~ ATNF        Exchange NAS~ 1-10 Emp~ Based i~ \"180 Life ~\n 9 1847 Holdings LLC    EFSH        Exchange NYS~ 1-10 Emp~ Based i~ \"1847 Hold~\n10 1895 Bancorp of Wis~ BCOW        Exchange NAS~ 51-200 E~ Based i~ \"1895 Banc~\n# i 67 more rows\n# i 5 more variables: company_website &lt;chr&gt;, facebook &lt;chr&gt;, youtube &lt;chr&gt;,\n#   linkedin &lt;chr&gt;, twitter &lt;chr&gt;\n\n\nExplanation:\n\nThis is similar to the previous block but focuses on extracting the annual report links for each company.\n\n\ntab_company_reports &lt;- purrr::map(tab_log_companies$path, ~ get_company_reports(read_raw_html(.x)), .progress = TRUE)\n\n =======================&gt;-------   77% |  ETA:  1s\n\ntab_company_reports &lt;- dplyr::bind_rows(tab_company_reports)\ntab_company_reports\n\n# A tibble: 842 x 3\n   heading             year report_link                                         \n   &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;                                               \n 1 2021 Annual Report  2021 https://www.annualreports.com/HostedData/AnnualRepo~\n 2 2020 Annual Report  2020 https://www.annualreports.com/HostedData/AnnualRepo~\n 3 2019 Annual Report  2019 https://www.annualreports.com/HostedData/AnnualRepo~\n 4 2018 Annual Report  2018 https://www.annualreports.com/HostedData/AnnualRepo~\n 5 2017 Annual Report  2017 https://www.annualreports.com/HostedData/AnnualRepo~\n 6 2016 Annual Report  2016 https://www.annualreports.com/HostedData/AnnualRepo~\n 7 2021 Annual Report  2021 https://www.annualreports.com/HostedData/AnnualRepo~\n 8 2020 Annual Report  2020 https://www.annualreports.com/HostedData/AnnualRepo~\n 9 2019 Annual Report  2019 https://www.annualreports.com/HostedData/AnnualRepo~\n10 2018 Annual Report  2018 https://www.annualreports.com/HostedData/AnnualRepo~\n# i 832 more rows"
  },
  {
    "objectID": "exercises/day2.html#download-the-annual-reports",
    "href": "exercises/day2.html#download-the-annual-reports",
    "title": "Day 2 - Web Scraping",
    "section": "Download the Annual Reports",
    "text": "Download the Annual Reports\nExplanation:\n\nThis function, download_pdf, is designed to download a PDF from a given URL and save it to a specified directory.\nIt first checks if the file already exists in the directory. If it does, the function returns a message and doesn’t download the file again.\nIf the file doesn’t exist, the function attempts to download the PDF. If the download is successful, details about the download (like the start time and duration) are logged in a tibble.\nIf there’s an error during the download (e.g., the URL doesn’t point to a PDF), the error is caught and logged.\nThe function also writes a log to a CSV file, which can be useful for tracking and debugging.\n\n\ndownload_pdf &lt;- function(.url, .dir, .wait = 1) {\n  # Create the directory if it doesn't exist\n  dir_ &lt;- fs::dir_create(.dir, \"pdf\")\n  \n  # Extract the filename from the URL\n  id_ &lt;- basename(.url)\n  fil_ &lt;- file.path(dir_, id_)\n  \n  # Check if the file already exists\n  if (file.exists(fil_)) {\n    return(NULL)\n  }\n  \n  # Create a log tibble\n  tab_log_ &lt;- tibble::tibble(\n    file_id = id_,\n    file_link = .url,\n    path = fil_,\n    err = FALSE,\n    err_msg = NA_character_,\n    start_time = NA_real_,\n    time = NA_real_\n  )\n  \n  # Try to download the PDF\n  tryCatch(\n    {\n      start_time_ &lt;- Sys.time()\n      response &lt;- httr::GET(.url, httr::write_disk(fil_, overwrite = TRUE))\n      \n      # Check if the response content type is a PDF\n      if (httr::http_type(response) != \"application/pdf\") {\n        stop(\"The URL does not point to a PDF.\")\n      }\n      \n      end_time_ &lt;- Sys.time()\n      Sys.sleep(.wait)\n      tab_log_ &lt;- tab_log_ %&gt;%\n        dplyr::mutate(\n          start_time = start_time_,\n          time = difftime(end_time_, start_time_, units = \"secs\")\n        )\n    },\n    error = function(e) {\n      tab_log_ &lt;- dplyr::mutate(tab_log_, err = TRUE, err_msg = e$message)\n      return(tab_log_)\n    }\n  )\n  \n  # Write the log to a CSV file\n  log_file &lt;- file.path(.dir, \"log.csv\")\n  if (!file.exists(log_file)) {\n    readr::write_delim(tab_log_, log_file, \"|\", na = \"\")\n  } else {\n    readr::write_delim(tab_log_, log_file, \"|\", na = \"\", append = TRUE)\n  }\n}\n\nDownload the first 100 reports\n\npurrr::walk(tab_company_reports$report_link[1:100], ~ download_pdf(.x, \"day2/pdfs\"), .progress = TRUE)\n\nExplanation:\n\npurrr::walk is used to apply the download_pdf function to the first 100 report links in tab_company_reports.\nEach report link is passed as .x to the download_pdf function, which then attempts to download the PDF and save it to the “day2/pdfs” directory.\nThe .progress = TRUE argument provides a progress bar, which is helpful when downloading multiple files."
  },
  {
    "objectID": "exercises/day0.html",
    "href": "exercises/day0.html",
    "title": "Day 0 - Making Sure Everything Works",
    "section": "",
    "text": "Setting Up R and Rstudio\nR and RStudio installations are needed for our interactive exercises (I would recommend installing the newest versions of both R and Rstudio). The setup is intuitive. Kindly follow the instructions to install the necessary components.\n\n\nDownloading the Public GitHub Repository\n\nOpen the Repository in a Web Browser:\n\nNavigate to the provided URL: https://github.com/MatthiasUckert/RpMMA24-Code\n\nFind the ‘Code’ Button:\n\nOn the main page of the repository, you’ll see a green button labeled “Code”.\n\nDownload as ZIP:\n\nClick on the “Code” button.\nIn the dropdown menu, you’ll see an option to “Download ZIP”. Click on this option.\nThis will start downloading the entire repository as a ZIP file to your computer.\n\nExtract the ZIP File:\n\nOnce the download is complete, navigate to the location where the ZIP file was saved (typically the “Downloads” folder).\nRight-click on the ZIP file (named RpMMA24-Code-main.zip or similar) and choose the option to “Extract All” (the exact wording might vary depending on your operating system).\nChoose a destination folder for the extracted files and click “Extract”.\n\nOpen the R Project:\n\nNavigate to the extracted folder.\nInside, you should find a file with the .Rproj extension, which represents the R Project.\nIf you have RStudio installed, you can double-click on this .Rproj file to open the project in RStudio. If not, you can still navigate through the folder and open individual R scripts using any text editor or R itself.\n\n\n\n\nRestoring the Project from the renv Lockfile\nIn this section, we will restore the project’s R packages using the renv.lock file. This ensures that all students have the same package versions for consistency and reproducibility.\n\nMake sure you have the renv package installed.\n\n\ninstall.packages(\"renv\")\n\n\nInitialize the renv project with this command:\n\n\nrenv::init()\n\nYou should see the following output:\nThis project already has a lockfile. What would you like to do?\n1: Restore the project from the lockfile.\n2: Discard the lockfile and re-initialize the project.\n3: Activate the project without snapshotting or installing any packages.\n4: Abort project initialization.\nSelection:\nType 1 and click enter\nThis should now install all necessary dependencies onto you computer (depending on your internet connection and computer specs this process might take 5 minutes or more)"
  }
]